{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b1a21845-5edc-410a-ab41-5196319ddec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3786fdb-7af6-4ce2-b730-90c60a5d896f",
   "metadata": {},
   "source": [
    "# Regression Models Lab\n",
    "## Linear and logistic regression: theory and practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6ff4b-e18a-4580-8c7c-54f4c7ef2dc9",
   "metadata": {},
   "source": [
    "In this lab you'll revisit and expand on your knowledge of modelling in general, as well as the fundamentals of linear and logistic regression. As a reminder, _linear regression_ is a regression model (regressor), and _logistic regression_ is a classification model (classifier).\n",
    "\n",
    "This time, you'll use generated data, in order to separate some of the complexity of handling various datasets from inspecting and evaluating models.\n",
    "\n",
    "**Use vectorization as much as possible!** You should be able to complete the lab using for-loops only to track the training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a9603-c803-4728-a69d-b6acfe2bad8a",
   "metadata": {},
   "source": [
    "### Problem 1. Generate some data for multiple linear regression (1 point)\n",
    "As an expansion to the lecture, you'll create a dataset and a model.\n",
    "\n",
    "Create a dataset of some (e.g., 50-500) observations of several (e.g., 5-20) independent features. You can use random generators for them; think about what distributions you'd like to use. Let's call them $x_1, x_2, ..., x_m$. The data matrix $X$ you should get should be of size $n \\times m$. It's best if all features have different ranges.\n",
    "\n",
    "Create the dependent variable by assigning coefficients $\\bar{a_1}, \\bar{a_2}, ..., \\bar{a_m}, \\bar{b}$ and calculating $y$ as a linear combination of the input features. Add some random noise to the functional values. I've used bars over coefficients to avoid confusion with the model parameters later.\n",
    "\n",
    "Save the dataset ($X$ and $y$), and \"forget\" that the coefficients have ever existed. \"All\" you have is the file and the implicit assumption that there is a linear relationship between $X$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c12e9a-92f0-4b83-8dfa-cc101cffe882",
   "metadata": {},
   "source": [
    "1. Lets create a function that generates a synthetic dataset for linear regression by creating random features with specified distributions. We will have 100 observations(rows) and 10 features(columns) with some random noise to make it like a real life scenario. We will set some seed, which means we can generate the same random numbers every time we run the function. We will generate random features using normal and uniform distribution to make it random but not too noisy. Also we will create an array of values representing the weight or importance of each feature in the linear equation and add a constat term(bias) to the linear equation. And then we will save the generated dataset into a csv file that we will later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f82d3ed2-0be7-4b9a-b893-6b955e6e8191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(seed=49, n=100, m=10, noise_std=0.5, file_name='linear_regression_data.csv'):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset for multiple linear regression with the given parameters and saves it to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    seed : int, optional\n",
    "        Seed for random number generation to ensure reproducibility (default is 49).\n",
    "    n : int, optional\n",
    "        Number of observations (default is 100).\n",
    "    m : int, optional\n",
    "        Number of features (default is 10).\n",
    "    noise_std : float, optional\n",
    "        Standard deviation of the noise added to the dependent variable `y` (default is 0.5).\n",
    "    file_name : str, optional\n",
    "        Name of the CSV file to save the generated dataset (default is 'linear_regression_data.csv').\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Set a seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate random features with different ranges and distributions\n",
    "    X = np.column_stack([\n",
    "        np.random.uniform(0, 50, n),         # Feature 1: Uniform distribution [0, 50]\n",
    "        np.random.normal(10, 5, n),          # Feature 2: Normal distribution mean=10, std=5\n",
    "        np.random.normal(20, 10, n),         # Feature 3: Normal distribution mean=20, std=10\n",
    "        np.random.uniform(0, 100, n),        # Feature 4: Uniform distribution [0, 100]\n",
    "        np.random.uniform(-50, 50, n),       # Feature 5: Uniform distribution [-50, 50]\n",
    "        np.random.normal(100, 20, n),        # Feature 6: Normal distribution mean=100, std=20\n",
    "        np.random.uniform(0, 1, n),          # Feature 7: Uniform distribution [0, 1]\n",
    "        np.random.normal(50, 15, n),         # Feature 8: Normal distribution mean=50, std=15\n",
    "        np.random.uniform(-10, 10, n),       # Feature 9: Uniform distribution [-10, 10]\n",
    "        np.random.normal(0, 5, n)            # Feature 10: Normal distribution mean=0, std=5\n",
    "    ])\n",
    "\n",
    "    # Define coefficients and calculate y\n",
    "    coefficients = np.array([5, 2, 1, 3, -1, 4, 0.5, 7, 1, 6])  # Coefficients\n",
    "    bias = 10  # Intercept term\n",
    "\n",
    "    # Calculate y as a linear combination of features + reduced noise\n",
    "    noise = np.random.normal(0, noise_std, n)\n",
    "    y = X.dot(coefficients) + bias + noise\n",
    "\n",
    "    # Combine X and y into a DataFrame\n",
    "    columns = [f'x{i+1}' for i in range(m)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    df['y'] = y\n",
    "\n",
    "    # Save to a CSV file\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Example usage\n",
    "generate_linear_regression_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58feb68-eb93-43a4-8145-d2be1d44ae95",
   "metadata": {},
   "source": [
    "2. The dependant variable is a linear combination of the features each multiplioed by its coefficient. The bias and noise are added to shit the result and add some randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb4164-eaa3-4b76-ae3f-36da95416b7b",
   "metadata": {},
   "source": [
    "### Problem 2. Check your assumption (1 point)\n",
    "Read the dataset you just saved (this is just to simulate starting a new project). It's a good idea to test and verify our assumptions. Find a way to check whether there really is a linear relationship between the features and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef670b4-df53-47fc-8ba9-cd2e25ea5297",
   "metadata": {},
   "source": [
    "1. Lets read the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d13c8452-5eb1-4719-88da-372331cfce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('linear_regression_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a9145e17-994a-4947-8e86-4904fce47127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.048223</td>\n",
       "      <td>14.684972</td>\n",
       "      <td>36.750488</td>\n",
       "      <td>17.647882</td>\n",
       "      <td>-1.352161</td>\n",
       "      <td>105.003606</td>\n",
       "      <td>0.032418</td>\n",
       "      <td>36.675582</td>\n",
       "      <td>6.454816</td>\n",
       "      <td>-0.177988</td>\n",
       "      <td>888.434030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.353092</td>\n",
       "      <td>12.976214</td>\n",
       "      <td>5.786552</td>\n",
       "      <td>62.503953</td>\n",
       "      <td>-18.537786</td>\n",
       "      <td>97.126505</td>\n",
       "      <td>0.728675</td>\n",
       "      <td>57.590926</td>\n",
       "      <td>2.687929</td>\n",
       "      <td>-0.954695</td>\n",
       "      <td>1097.794115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.316757</td>\n",
       "      <td>2.564661</td>\n",
       "      <td>21.969222</td>\n",
       "      <td>16.107910</td>\n",
       "      <td>-9.539181</td>\n",
       "      <td>79.347032</td>\n",
       "      <td>0.384974</td>\n",
       "      <td>38.374836</td>\n",
       "      <td>2.353192</td>\n",
       "      <td>5.447479</td>\n",
       "      <td>948.216392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.580172</td>\n",
       "      <td>2.555282</td>\n",
       "      <td>12.609197</td>\n",
       "      <td>3.294174</td>\n",
       "      <td>34.678986</td>\n",
       "      <td>114.049408</td>\n",
       "      <td>0.670229</td>\n",
       "      <td>61.486362</td>\n",
       "      <td>-8.803386</td>\n",
       "      <td>-2.271465</td>\n",
       "      <td>1091.099310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.163838</td>\n",
       "      <td>6.667057</td>\n",
       "      <td>12.419748</td>\n",
       "      <td>7.190539</td>\n",
       "      <td>1.390041</td>\n",
       "      <td>98.397612</td>\n",
       "      <td>0.946322</td>\n",
       "      <td>34.952584</td>\n",
       "      <td>9.621198</td>\n",
       "      <td>3.755145</td>\n",
       "      <td>898.384264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>24.507101</td>\n",
       "      <td>5.777868</td>\n",
       "      <td>10.485026</td>\n",
       "      <td>82.451139</td>\n",
       "      <td>-49.397967</td>\n",
       "      <td>91.530256</td>\n",
       "      <td>0.992329</td>\n",
       "      <td>58.431917</td>\n",
       "      <td>5.836962</td>\n",
       "      <td>2.449839</td>\n",
       "      <td>1248.364772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>30.333632</td>\n",
       "      <td>9.755933</td>\n",
       "      <td>10.380063</td>\n",
       "      <td>89.426530</td>\n",
       "      <td>-37.727605</td>\n",
       "      <td>95.386657</td>\n",
       "      <td>0.464687</td>\n",
       "      <td>50.084078</td>\n",
       "      <td>9.005066</td>\n",
       "      <td>0.880429</td>\n",
       "      <td>1244.188832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.556174</td>\n",
       "      <td>17.423124</td>\n",
       "      <td>26.218228</td>\n",
       "      <td>59.665379</td>\n",
       "      <td>32.063724</td>\n",
       "      <td>127.598291</td>\n",
       "      <td>0.255543</td>\n",
       "      <td>54.456345</td>\n",
       "      <td>8.565428</td>\n",
       "      <td>-3.030257</td>\n",
       "      <td>1102.670795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>19.055393</td>\n",
       "      <td>9.506344</td>\n",
       "      <td>26.749117</td>\n",
       "      <td>35.451330</td>\n",
       "      <td>13.090141</td>\n",
       "      <td>117.316016</td>\n",
       "      <td>0.604384</td>\n",
       "      <td>60.580384</td>\n",
       "      <td>-1.494528</td>\n",
       "      <td>-6.006924</td>\n",
       "      <td>1099.823819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>17.970989</td>\n",
       "      <td>6.813711</td>\n",
       "      <td>25.330519</td>\n",
       "      <td>53.631678</td>\n",
       "      <td>-12.809113</td>\n",
       "      <td>108.721585</td>\n",
       "      <td>0.499013</td>\n",
       "      <td>29.531854</td>\n",
       "      <td>-1.046319</td>\n",
       "      <td>0.857785</td>\n",
       "      <td>958.691260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1         x2         x3         x4         x5          x6  \\\n",
       "0   15.048223  14.684972  36.750488  17.647882  -1.352161  105.003606   \n",
       "1   12.353092  12.976214   5.786552  62.503953 -18.537786   97.126505   \n",
       "2   46.316757   2.564661  21.969222  16.107910  -9.539181   79.347032   \n",
       "3   44.580172   2.555282  12.609197   3.294174  34.678986  114.049408   \n",
       "4   34.163838   6.667057  12.419748   7.190539   1.390041   98.397612   \n",
       "..        ...        ...        ...        ...        ...         ...   \n",
       "95  24.507101   5.777868  10.485026  82.451139 -49.397967   91.530256   \n",
       "96  30.333632   9.755933  10.380063  89.426530 -37.727605   95.386657   \n",
       "97   0.556174  17.423124  26.218228  59.665379  32.063724  127.598291   \n",
       "98  19.055393   9.506344  26.749117  35.451330  13.090141  117.316016   \n",
       "99  17.970989   6.813711  25.330519  53.631678 -12.809113  108.721585   \n",
       "\n",
       "          x7         x8        x9       x10            y  \n",
       "0   0.032418  36.675582  6.454816 -0.177988   888.434030  \n",
       "1   0.728675  57.590926  2.687929 -0.954695  1097.794115  \n",
       "2   0.384974  38.374836  2.353192  5.447479   948.216392  \n",
       "3   0.670229  61.486362 -8.803386 -2.271465  1091.099310  \n",
       "4   0.946322  34.952584  9.621198  3.755145   898.384264  \n",
       "..       ...        ...       ...       ...          ...  \n",
       "95  0.992329  58.431917  5.836962  2.449839  1248.364772  \n",
       "96  0.464687  50.084078  9.005066  0.880429  1244.188832  \n",
       "97  0.255543  54.456345  8.565428 -3.030257  1102.670795  \n",
       "98  0.604384  60.580384 -1.494528 -6.006924  1099.823819  \n",
       "99  0.499013  29.531854 -1.046319  0.857785   958.691260  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1ea5e-f0fe-4d75-ae40-d7a85bd50613",
   "metadata": {},
   "source": [
    "2. Let's look at the summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "328a48ee-a8f7-4fc4-a237-8a5790f02507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.662134</td>\n",
       "      <td>9.343783</td>\n",
       "      <td>21.152105</td>\n",
       "      <td>50.354324</td>\n",
       "      <td>-2.395422</td>\n",
       "      <td>101.435863</td>\n",
       "      <td>0.515018</td>\n",
       "      <td>49.671265</td>\n",
       "      <td>-0.004639</td>\n",
       "      <td>-0.294134</td>\n",
       "      <td>1093.565423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.364536</td>\n",
       "      <td>4.785074</td>\n",
       "      <td>10.901420</td>\n",
       "      <td>31.740924</td>\n",
       "      <td>30.235295</td>\n",
       "      <td>18.366215</td>\n",
       "      <td>0.295680</td>\n",
       "      <td>15.641905</td>\n",
       "      <td>6.226536</td>\n",
       "      <td>4.927380</td>\n",
       "      <td>190.834720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.264848</td>\n",
       "      <td>-1.800488</td>\n",
       "      <td>-9.904430</td>\n",
       "      <td>0.046961</td>\n",
       "      <td>-49.397967</td>\n",
       "      <td>57.600209</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>13.515023</td>\n",
       "      <td>-9.786720</td>\n",
       "      <td>-11.768953</td>\n",
       "      <td>650.106886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.492661</td>\n",
       "      <td>6.246821</td>\n",
       "      <td>14.350854</td>\n",
       "      <td>18.349670</td>\n",
       "      <td>-25.803329</td>\n",
       "      <td>87.956508</td>\n",
       "      <td>0.254806</td>\n",
       "      <td>38.727170</td>\n",
       "      <td>-6.275782</td>\n",
       "      <td>-3.319498</td>\n",
       "      <td>935.265471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29.874864</td>\n",
       "      <td>9.414536</td>\n",
       "      <td>22.414765</td>\n",
       "      <td>49.371951</td>\n",
       "      <td>-5.245765</td>\n",
       "      <td>100.428902</td>\n",
       "      <td>0.530849</td>\n",
       "      <td>49.604450</td>\n",
       "      <td>0.474872</td>\n",
       "      <td>-0.800208</td>\n",
       "      <td>1098.808967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.145108</td>\n",
       "      <td>12.264674</td>\n",
       "      <td>29.324854</td>\n",
       "      <td>81.640298</td>\n",
       "      <td>27.539355</td>\n",
       "      <td>114.308294</td>\n",
       "      <td>0.741423</td>\n",
       "      <td>59.317962</td>\n",
       "      <td>5.867380</td>\n",
       "      <td>2.318953</td>\n",
       "      <td>1225.821286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49.505357</td>\n",
       "      <td>23.996049</td>\n",
       "      <td>49.210025</td>\n",
       "      <td>99.366850</td>\n",
       "      <td>46.693497</td>\n",
       "      <td>150.511291</td>\n",
       "      <td>0.992329</td>\n",
       "      <td>107.039048</td>\n",
       "      <td>9.892695</td>\n",
       "      <td>14.195242</td>\n",
       "      <td>1540.514971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2          x3          x4          x5          x6  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean    27.662134    9.343783   21.152105   50.354324   -2.395422  101.435863   \n",
       "std     14.364536    4.785074   10.901420   31.740924   30.235295   18.366215   \n",
       "min      0.264848   -1.800488   -9.904430    0.046961  -49.397967   57.600209   \n",
       "25%     15.492661    6.246821   14.350854   18.349670  -25.803329   87.956508   \n",
       "50%     29.874864    9.414536   22.414765   49.371951   -5.245765  100.428902   \n",
       "75%     40.145108   12.264674   29.324854   81.640298   27.539355  114.308294   \n",
       "max     49.505357   23.996049   49.210025   99.366850   46.693497  150.511291   \n",
       "\n",
       "               x7          x8          x9         x10            y  \n",
       "count  100.000000  100.000000  100.000000  100.000000   100.000000  \n",
       "mean     0.515018   49.671265   -0.004639   -0.294134  1093.565423  \n",
       "std      0.295680   15.641905    6.226536    4.927380   190.834720  \n",
       "min      0.003197   13.515023   -9.786720  -11.768953   650.106886  \n",
       "25%      0.254806   38.727170   -6.275782   -3.319498   935.265471  \n",
       "50%      0.530849   49.604450    0.474872   -0.800208  1098.808967  \n",
       "75%      0.741423   59.317962    5.867380    2.318953  1225.821286  \n",
       "max      0.992329  107.039048    9.892695   14.195242  1540.514971  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f44a9-cdc8-4c7d-be86-ddef774f38fe",
   "metadata": {},
   "source": [
    "3. Overall, the data appears to be well-spread, with varying ranges and distributions across different features. This kind of variation is beneficial for linear regression models because it helps in determining the relationships between different features and the target variable (y). For instance, x6 and x7 have relatively higher mean values and lower standard deviations, indicating that these variables might have a more stable and consistent relationship with y.\n",
    "If you look closely at y, the dependent variable, you'll notice that its mean is high (around 1093.57), and its range is wide (from 650.11 to 1540.51). This indicates that the linear combination of features plus the bias and noise leads to a fairly broad range of outcomes.\n",
    "4. Let's look at the correlation between different features and the correlations between feature and the linear combination y and plot a heatmap to see the correlation matrix to see what is the correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6177af0f-8470-4c1a-9e89-0ec4f603f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dd2747c4-73b9-4fef-9352-b5c64f4ab1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.183071</td>\n",
       "      <td>-0.224689</td>\n",
       "      <td>-0.128034</td>\n",
       "      <td>0.037954</td>\n",
       "      <td>-0.032891</td>\n",
       "      <td>0.028195</td>\n",
       "      <td>0.086744</td>\n",
       "      <td>-0.052082</td>\n",
       "      <td>0.048093</td>\n",
       "      <td>0.327457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>-0.183071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.121106</td>\n",
       "      <td>0.150839</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.104358</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.078673</td>\n",
       "      <td>0.019969</td>\n",
       "      <td>-0.144729</td>\n",
       "      <td>0.112081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>-0.224689</td>\n",
       "      <td>-0.121106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.046577</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>-0.045621</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>-0.089840</td>\n",
       "      <td>-0.052464</td>\n",
       "      <td>0.114309</td>\n",
       "      <td>-0.113364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>-0.128034</td>\n",
       "      <td>0.150839</td>\n",
       "      <td>-0.046577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.253957</td>\n",
       "      <td>0.098432</td>\n",
       "      <td>-0.013676</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.126413</td>\n",
       "      <td>-0.078009</td>\n",
       "      <td>0.565716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5</th>\n",
       "      <td>0.037954</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>-0.253957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.124524</td>\n",
       "      <td>0.206327</td>\n",
       "      <td>-0.081762</td>\n",
       "      <td>-0.068232</td>\n",
       "      <td>0.070357</td>\n",
       "      <td>-0.259436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x6</th>\n",
       "      <td>-0.032891</td>\n",
       "      <td>0.104358</td>\n",
       "      <td>-0.045621</td>\n",
       "      <td>0.098432</td>\n",
       "      <td>0.124524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008157</td>\n",
       "      <td>0.065699</td>\n",
       "      <td>0.184709</td>\n",
       "      <td>-0.109906</td>\n",
       "      <td>0.431302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x7</th>\n",
       "      <td>0.028195</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>-0.013676</td>\n",
       "      <td>0.206327</td>\n",
       "      <td>-0.008157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012629</td>\n",
       "      <td>0.065558</td>\n",
       "      <td>0.081698</td>\n",
       "      <td>-0.024728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x8</th>\n",
       "      <td>0.086744</td>\n",
       "      <td>0.078673</td>\n",
       "      <td>-0.089840</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.081762</td>\n",
       "      <td>0.065699</td>\n",
       "      <td>-0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124715</td>\n",
       "      <td>-0.069355</td>\n",
       "      <td>0.662532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x9</th>\n",
       "      <td>-0.052082</td>\n",
       "      <td>0.019969</td>\n",
       "      <td>-0.052464</td>\n",
       "      <td>0.126413</td>\n",
       "      <td>-0.068232</td>\n",
       "      <td>0.184709</td>\n",
       "      <td>0.065558</td>\n",
       "      <td>-0.124715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047856</td>\n",
       "      <td>0.077109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x10</th>\n",
       "      <td>0.048093</td>\n",
       "      <td>-0.144729</td>\n",
       "      <td>0.114309</td>\n",
       "      <td>-0.078009</td>\n",
       "      <td>0.070357</td>\n",
       "      <td>-0.109906</td>\n",
       "      <td>0.081698</td>\n",
       "      <td>-0.069355</td>\n",
       "      <td>-0.047856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.327457</td>\n",
       "      <td>0.112081</td>\n",
       "      <td>-0.113364</td>\n",
       "      <td>0.565716</td>\n",
       "      <td>-0.259436</td>\n",
       "      <td>0.431302</td>\n",
       "      <td>-0.024728</td>\n",
       "      <td>0.662532</td>\n",
       "      <td>0.077109</td>\n",
       "      <td>0.039298</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5        x6        x7  \\\n",
       "x1   1.000000 -0.183071 -0.224689 -0.128034  0.037954 -0.032891  0.028195   \n",
       "x2  -0.183071  1.000000 -0.121106  0.150839  0.004209  0.104358  0.027239   \n",
       "x3  -0.224689 -0.121106  1.000000 -0.046577  0.021235 -0.045621 -0.040039   \n",
       "x4  -0.128034  0.150839 -0.046577  1.000000 -0.253957  0.098432 -0.013676   \n",
       "x5   0.037954  0.004209  0.021235 -0.253957  1.000000  0.124524  0.206327   \n",
       "x6  -0.032891  0.104358 -0.045621  0.098432  0.124524  1.000000 -0.008157   \n",
       "x7   0.028195  0.027239 -0.040039 -0.013676  0.206327 -0.008157  1.000000   \n",
       "x8   0.086744  0.078673 -0.089840  0.069824 -0.081762  0.065699 -0.012629   \n",
       "x9  -0.052082  0.019969 -0.052464  0.126413 -0.068232  0.184709  0.065558   \n",
       "x10  0.048093 -0.144729  0.114309 -0.078009  0.070357 -0.109906  0.081698   \n",
       "y    0.327457  0.112081 -0.113364  0.565716 -0.259436  0.431302 -0.024728   \n",
       "\n",
       "           x8        x9       x10         y  \n",
       "x1   0.086744 -0.052082  0.048093  0.327457  \n",
       "x2   0.078673  0.019969 -0.144729  0.112081  \n",
       "x3  -0.089840 -0.052464  0.114309 -0.113364  \n",
       "x4   0.069824  0.126413 -0.078009  0.565716  \n",
       "x5  -0.081762 -0.068232  0.070357 -0.259436  \n",
       "x6   0.065699  0.184709 -0.109906  0.431302  \n",
       "x7  -0.012629  0.065558  0.081698 -0.024728  \n",
       "x8   1.000000 -0.124715 -0.069355  0.662532  \n",
       "x9  -0.124715  1.000000 -0.047856  0.077109  \n",
       "x10 -0.069355 -0.047856  1.000000  0.039298  \n",
       "y    0.662532  0.077109  0.039298  1.000000  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "428c3a81-d64c-4349-a717-0deee4c4e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239511e8-0432-4cf8-a4ff-f2d7c96ce463",
   "metadata": {},
   "source": [
    "5. From this heatmap we can see that features like x8, x4, and x6 are likely to be the most important predictors in a linear regression model due to their relatively high positive correlations with y. x5 has a notable negative correlation with y, which may suggest an inverse relationship. Some features have low or no correlation with y, indicating they might not significantly contribute to the model or may require interaction with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a7cc3fb0-7037-4dd3-be93-f24aef5df294",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_y = correlation['y'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c2227b9b-d16a-4e06-a200-c316384b42d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y      1.000000\n",
       "x8     0.662532\n",
       "x4     0.565716\n",
       "x6     0.431302\n",
       "x1     0.327457\n",
       "x2     0.112081\n",
       "x9     0.077109\n",
       "x10    0.039298\n",
       "x7    -0.024728\n",
       "x3    -0.113364\n",
       "x5    -0.259436\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_with_y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf7f88-2aab-402b-95c7-62bc6cd5380d",
   "metadata": {},
   "source": [
    "6. The results indicate that there is indeed a linear relationship between certain features and the output y. The features with higher coefficients in the data generation process (e.g., x8, x4, x6) exhibit stronger correlations with y, confirming that these features have a more significant linear relationship with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f9a49-cee6-4537-b82f-d8148a45c0c2",
   "metadata": {},
   "source": [
    "### Problem 3. Figure out the modelling function (1 point)\n",
    "The modelling function for linear regression is of the form\n",
    "$$ \\tilde{y} = \\sum_{i=1}^{m}a_i x_i + b $$\n",
    "\n",
    "If you want to be clever, you can find a way to represent $b$ in the same way as the other coefficients.\n",
    "\n",
    "Write a Python function which accepts coefficients and data, and ensure (test) it works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53e8da-57ea-47c0-a60c-f1bc92952490",
   "metadata": {},
   "source": [
    "1. Let's first understand the function. This equation represents a linear combination of the input features $x_i$ with their corresponding coefficients $a_i$ plus an intercept term $b$. The intercept shifts the prediciton up or down, while the coefficients determine the contribution of each feature to the prediction.\n",
    "2. To make the repersentation more uniform we can include $b$ as a coefficient by adding 'bias' feature to the input data. This bias feature is a column of ones.\n",
    "3. Now let's create e function that predicts the output using a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0800a26d-3204-49ad-8a83-37edf0f4128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_predict(X, coefficients):\n",
    "    \"\"\"\n",
    "    Predict the output using a linear regression model with the intercept represented as a coefficient.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        np.ndarray: The predicted output vector of shape (n,).\n",
    "    \"\"\"\n",
    "    # Add a bias column (column of ones) to X for the intercept term\n",
    "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    # Compute the predicted output as a linear combination of inputs and coefficients\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Example Test Function\n",
    "def test_linear_regression_predict():\n",
    "    # Create some test data\n",
    "    X_test = np.array([\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6]\n",
    "    ])\n",
    "    \n",
    "    # Define coefficients (for example b=1, a1=2, a2=3)\n",
    "    coefficients_test = np.array([1, 2, 3])\n",
    "    \n",
    "    # Expected output y = 1*1 + 2*x1 + 3*x2\n",
    "    expected_y = np.array([9, 19, 29])\n",
    "    \n",
    "    # Predict using the function\n",
    "    y_pred = linear_regression_predict(X_test, coefficients_test)\n",
    "    \n",
    "    # Check if the prediction matches the expected output\n",
    "    assert np.allclose(y_pred, expected_y), \"Test failed!\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test function\n",
    "test_linear_regression_predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182200f9-6254-48ba-9f08-2a25d34568b0",
   "metadata": {},
   "source": [
    "4. This function predicts the output (y) for a given set of input data (X) using a linear regression model. The linear regression model is represented by a set of coefficients, including an intercept (bias term). \n",
    "5. The first function computes predictions for a linear regression model. The second one correctly computes these predictions by comparing them against manually computed expected values and prints out the test result(if y_pred matches expected_y it passes).\n",
    "6. As we can see the function seems to be working with the result being positive.\n",
    "7. Let's now see how it works on our synthetic dataset and to see the first 5 predictions and actual Y's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6c2cf65f-06c0-45a4-803f-d0ab75b627af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Predictions: [ 887.80395368 1098.52068715  947.78776956 1090.32925997  897.63838757]\n",
      "First 5 Actual y: [ 888.43403034 1097.79411467  948.2163917  1091.09930987  898.3842639 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('linear_regression_data.csv')\n",
    "X = df.iloc[:, :-1].values  # All columns except the last one (features)\n",
    "y = df['y'].values  # Last column (target)\n",
    "\n",
    "# Define the coefficients (including the intercept)\n",
    "coefficients = np.array([10, 5, 2, 1, 3, -1, 4, 0.5, 7, 1, 6])\n",
    "\n",
    "# Predict using the linear_regression_predict function\n",
    "y_pred = linear_regression_predict(X, coefficients)\n",
    "\n",
    "# Compare the prediction with the actual y values\n",
    "print(\"First 5 Predictions:\", y_pred[:5])\n",
    "print(\"First 5 Actual y:\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f731009-f09c-46b6-bb30-4c398c194d13",
   "metadata": {},
   "source": [
    "8. As we can see the predictions aren't 100% accurate but they are pretty close to the expected output.The predicted values are very close to the actual values. The differences between each corresponding prediction and actual value are quite small, which indicates that the model is accurately capturing the relationship between the input features and the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9d650-0994-4ae8-bba9-3b8fdecbdd3e",
   "metadata": {},
   "source": [
    "### Problem 4. Write the cost function and compute its gradients (1 point)\n",
    "Use MSE as the cost function $J$. Find a way to compute, calculate, or derive its gradients w.r.t. the model parameters $a_1, ..., a_m, b$\n",
    "\n",
    "Note that computing the cost function value and its gradients are two separate operations. Quick reminder: use vectorization to compute all gradients (maybe with the exception of $\\frac{\\partial J}{\\partial b}$) at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0b9f0-7a3e-470d-9f76-43985b876c7b",
   "metadata": {},
   "source": [
    "1. Lets first understand the cost function. In linear regression its defined as MSE Mean Squared Error between the predicted and target values.\n",
    "2. Let's write a function that computes the cost and gradients. \n",
    "3. Cost Calculation: The function will calculate the MSE by summing the squared residuals between the predicted values and the actual values, then dividing by 2n.\n",
    "4. The gradients are computed using matrix multiplication. The key insight here is that by using vectorized operations you can compute the gradients for all coefficients (including the intercept) simultaneously.\n",
    "5. This implementation allows us to compute both the cost and its gradients efficiently using vectorization. The gradients can then be used to perform gradient descent or any other optimization algorithm to update the model's parameters and minimize the cost.\n",
    "6. We will first use dummy data to test the usage of the funciton and then will try it direcly on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "99388ae1-1b23-4233-acf9-daf589bba07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.0\n",
      "Gradients: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def compute_cost_and_gradients(X, y, coefficients):\n",
    "    \"\"\"\n",
    "    Compute the cost (MSE) and gradients for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        y (np.ndarray): The target values vector of shape (n,).\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        cost (float): The value of the cost function (MSE).\n",
    "        gradients (np.ndarray): The gradients of the cost function with respect to the coefficients\n",
    "                                of shape (m+1,).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]  # Number of observations\n",
    "    \n",
    "    # Add a bias column (column of ones) to X for the intercept term\n",
    "    X_with_bias = np.hstack((np.ones((n, 1)), X))\n",
    "    \n",
    "    # Compute the predictions\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    \n",
    "    # Compute the residuals\n",
    "    residuals = y_pred - y\n",
    "    \n",
    "    # Compute the cost (MSE)\n",
    "    cost = (1 / (2 * n)) * np.sum(residuals**2)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    gradients = (1 / n) * np.dot(X_with_bias.T, residuals)\n",
    "    \n",
    "    return cost, gradients\n",
    "\n",
    "# Example usage with dummy data\n",
    "X_test = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "y_test = np.array([9, 19, 29])\n",
    "coefficients_test = np.array([1, 2, 3])\n",
    "\n",
    "cost, gradients = compute_cost_and_gradients(X_test, y_test, coefficients_test)\n",
    "\n",
    "print(f\"Cost: {cost}\")\n",
    "print(f\"Gradients: {gradients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75722488-ac16-4dc2-a076-fda3890854f2",
   "metadata": {},
   "source": [
    "7. From what we got it looks like the predictions perfectly match the target values resulting in zero error. But let's now see what happens when we do it with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "dc1b229c-cf6b-4514-a507-5dd3b8020308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.18838955850016512\n",
      "Gradients: [-0.02631822 -1.08361025  0.12712699 -0.34526657 -0.1197014  -0.8141612\n",
      " -2.71445559 -0.00859682  1.50632271 -0.00296367 -0.62327643]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('linear_regression_data.csv')\n",
    "X = df.iloc[:, :-1].values  # All columns except the last one (features)\n",
    "y = df['y'].values  # Last column (target)\n",
    "\n",
    "# Define the coefficients (including the intercept)\n",
    "coefficients = np.array([10, 5, 2, 1, 3, -1, 4, 0.5, 7, 1, 6])\n",
    "\n",
    "# Compute the cost and gradients\n",
    "cost, gradients = compute_cost_and_gradients(X, y, coefficients)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Cost: {cost}\")\n",
    "print(f\"Gradients: {gradients}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca9c94-0279-40c7-9d06-99c0fbac9147",
   "metadata": {},
   "source": [
    "8. From what we got here we can see that the cost function value is relatively small, which suggests that the model's predictions are fairly close to the actual target values in our dataset. A cost of around 0.188 indicates a small average error, meaning the model is performing reasonably well.\n",
    "9. The gradient values you see correspond to the partial derivatives of the cost function with respect to each of the coefficients, including the intercept. These gradients indicate how much each coefficient should be adjusted to minimize the cost function. The direction and magnitude of the gradients suggest whether each coefficient should be increased or decreased to reduce the overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad786e-4299-4900-ae86-16cbd2235a6e",
   "metadata": {},
   "source": [
    "### Problem 5. Perform gradient descent (1 point)\n",
    "Perform weight updates iteratively. Find a useful criterion for stopping. For most cases, just using a fixed (large) number of steps is enough.\n",
    "\n",
    "You'll need to set a starting point (think about which one should be good, and how it matters); and a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88343859-8047-4df4-82a5-87ae9051aa40",
   "metadata": {},
   "source": [
    "1. We will create a function that implements gradient descent to optimize the coefficients of a linear regression model, which predicts an output variable y based on multiple input features X. The features will be normalized to have mean of 0 and std of 1. The learning rate we can vary but will start small something like 0.001 and we will set iterations around 1000 even though we could need less or more and we will define a tolerance level for early stopping. The coefficients, which include an intercept term, will be initialized to zero. These coefficients will be updated iteratively to minimize the cost function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dd4bc3bd-6b2c-4f40-9e94-40557a2325d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 615969.5230157449\n",
      "Iteration 100: Cost 503841.05682500947\n",
      "Iteration 200: Cost 412170.91686812724\n",
      "Iteration 300: Cost 337214.58580565255\n",
      "Iteration 400: Cost 275915.951183636\n",
      "Iteration 500: Cost 225779.95484686538\n",
      "Iteration 600: Cost 184769.00852915368\n",
      "Iteration 700: Cost 151218.68623749245\n",
      "Iteration 800: Cost 123769.0661653766\n",
      "Iteration 900: Cost 101308.78780186594\n",
      "Optimized Coefficients: [691.46642037  41.12008805   8.21576211  -4.44265742  62.28818438\n",
      " -24.96302083  48.42831875  -0.99814915  74.04427454   6.1765729\n",
      "  12.29028258]\n",
      "Final Cost: 83095.60036165424\n",
      "First 5 Predictions: [548.6273598  723.90220545 576.39212966 683.72797457 561.06516022]\n",
      "First 5 Actual y: [ 888.43403034 1097.79411467  948.2163917  1091.09930987  898.3842639 ]\n"
     ]
    }
   ],
   "source": [
    "# Function to compute cost and gradients for linear regression\n",
    "def compute_cost_and_gradients(X, y, coefficients):\n",
    "    \"\"\"\n",
    "    Compute the cost (MSE) and gradients for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        y (np.ndarray): The target values vector of shape (n,).\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        cost (float): The value of the cost function (MSE).\n",
    "        gradients (np.ndarray): The gradients of the cost function with respect to the coefficients\n",
    "                                of shape (m+1,).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]  # Number of observations\n",
    "    \n",
    "    # Add a bias column (column of ones) to X for the intercept term\n",
    "    X_with_bias = np.hstack((np.ones((n, 1)), X))\n",
    "    \n",
    "    # Compute the predictions\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    \n",
    "    # Compute the residuals\n",
    "    residuals = y_pred - y\n",
    "    \n",
    "    # Compute the cost (MSE)\n",
    "    cost = (1 / (2 * n)) * np.sum(residuals**2)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    gradients = (1 / n) * np.dot(X_with_bias.T, residuals)\n",
    "    \n",
    "    return cost, gradients\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('linear_regression_data.csv')\n",
    "X = df.iloc[:, :-1].values  # All columns except the last one (features)\n",
    "y = df['y'].values  # Last column (target)\n",
    "\n",
    "# Normalize features to have mean 0 and variance 1\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_normalized = (X - X_mean) / X_std\n",
    "\n",
    "# Define the learning rate and other parameters\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Initialize coefficients (including the intercept)\n",
    "coefficients = np.zeros(X_normalized.shape[1] + 1)  # Including intercept\n",
    "\n",
    "# Initialize an array to store the cost at each iteration\n",
    "cost_history = np.zeros(num_iterations)\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(num_iterations):\n",
    "    cost, gradients = compute_cost_and_gradients(X_normalized, y, coefficients)\n",
    "    coefficients -= learning_rate * gradients\n",
    "    \n",
    "    # Save cost to history\n",
    "    cost_history[i] = cost\n",
    "    \n",
    "    # Print cost every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: Cost {cost}\")\n",
    "    \n",
    "    # Early stopping if cost change is very small\n",
    "    if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n",
    "        print(f\"Converged at iteration {i}\")\n",
    "        break\n",
    "\n",
    "print(\"Optimized Coefficients:\", coefficients)\n",
    "print(\"Final Cost:\", cost_history[i])\n",
    "\n",
    "# Predict the output using the optimized coefficients\n",
    "def linear_regression_predict(X, coefficients):\n",
    "    \"\"\"\n",
    "    Predict the output using a linear regression model with the intercept represented as a coefficient.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        np.ndarray: The predicted output vector of shape (n,).\n",
    "    \"\"\"\n",
    "    # Add a bias column (column of ones) to X for the intercept term\n",
    "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    # Compute the predicted output as a linear combination of inputs and coefficients\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Predict the output with the final optimized coefficients\n",
    "y_pred = linear_regression_predict(X_normalized, coefficients)\n",
    "\n",
    "# Compare the prediction with the actual y values (first 5)\n",
    "print(\"First 5 Predictions:\", y_pred[:5])\n",
    "print(\"First 5 Actual y:\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c7666-f5a1-4064-81dc-62e65e2362d3",
   "metadata": {},
   "source": [
    "2. For each iteration, the cost (Mean Squared Error, MSE) and the gradients of the cost function with respect to the coefficients are computed.\n",
    "3. The coefficients are updated by subtracting the product of the learning rate and the gradients.\n",
    "4. The cost is stored in a history array to track the progress of gradient descent. The cost is printed every 100 iterations to monitor the optimization process.\n",
    "5. The algorithm includes an early stopping criterion based on the change in cost between iterations. If the change in cost is smaller than the defined tolerance, the algorithm stops early.\n",
    "6. The optimized coefficients, after gradient descent has converged, are printed along with the final cost.\n",
    "7. After running the code we can see that we got the optimized coefficients that we can use to make the gradiend descend again as we can see the predicitons aren't close to the actual y so it's not very accurate.\n",
    "8. Okay we will use the same function linear_regression_predict and alter the rest of the functions a bit. We will introduces feature normalization, where each feature is scaled to have a mean of 0 and a variance of 1. This is important in many machine learning algorithms, including gradient descent, to ensure that all features contribute equally to the learning process. Similar to the first code block, gradient descent will be performed, but with normalized features. The coefficients will be updated, and the cost will be tracked across iterations. The linear_regression_predict function is reused after training to make predictions using the optimized coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6d8bf9bf-f445-40c4-b60a-e379bd4ad58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 615969.5230157449\n",
      "Iteration 100: Cost 312.44214904502496\n",
      "Iteration 200: Cost 184.4013911064608\n",
      "Iteration 300: Cost 118.1304852674021\n",
      "Iteration 400: Cost 76.22194109103613\n",
      "Iteration 500: Cost 49.47620788030608\n",
      "Iteration 600: Cost 32.339097871809756\n",
      "Iteration 700: Cost 21.32418435862668\n",
      "Iteration 800: Cost 14.225197581831477\n",
      "Iteration 900: Cost 9.639098337944509\n",
      "First 5 Predictions: [ 886.13207852 1095.34208067  944.34024479 1094.47834939  893.05493059]\n",
      "First 5 Actual y: [ 888.43403034 1097.79411467  948.2163917  1091.09930987  898.3842639 ]\n",
      "Final Cost: 6.693865720669176\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_predict(X, coefficients):\n",
    "    \"\"\"\n",
    "    Predict the output using a linear regression model with the intercept represented as a coefficient.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        np.ndarray: The predicted output vector of shape (n,).\n",
    "    \"\"\"\n",
    "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    return y_pred\n",
    "\n",
    "def compute_cost_and_gradients(X, y, coefficients):\n",
    "    n = X.shape[0]\n",
    "    X_with_bias = np.hstack((np.ones((n, 1)), X))\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    residuals = y_pred - y\n",
    "    cost = (1 / (2 * n)) * np.sum(residuals**2)\n",
    "    gradients = (1 / n) * np.dot(X_with_bias.T, residuals)\n",
    "    return cost, gradients\n",
    "\n",
    "def gradient_descent(X, y, initial_coefficients, learning_rate=0.01, iterations=1000, tolerance=1e-6):\n",
    "    coefficients = initial_coefficients.copy()\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cost, gradients = compute_cost_and_gradients(X, y, coefficients)\n",
    "        coefficients -= learning_rate * gradients\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n",
    "            print(f\"Stopping early at iteration {i} due to minimal change in cost.\")\n",
    "            break\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "\n",
    "    return coefficients, cost_history\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('linear_regression_data.csv')\n",
    "X = df.iloc[:, :-1].values  # All columns except the last one (features)\n",
    "y = df['y'].values  # Last column (target)\n",
    "\n",
    "# Define initial coefficients (including the intercept)\n",
    "initial_coefficients = np.zeros(X.shape[1] + 1)\n",
    "\n",
    "# Run gradient descent with a new learning rate\n",
    "optimized_coefficients, cost_history = gradient_descent(X, y, initial_coefficients, learning_rate=0.0001, iterations=1000)\n",
    "\n",
    "# Predict using the optimized coefficients\n",
    "y_pred = linear_regression_predict(X, optimized_coefficients)\n",
    "\n",
    "# Compare the prediction with the actual y values\n",
    "print(\"First 5 Predictions:\", y_pred[:5])\n",
    "print(\"First 5 Actual y:\", y[:5])\n",
    "\n",
    "# Final cost\n",
    "print(f\"Final Cost: {cost_history[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7d0ba-1fe4-49fe-b2ed-daf67bc082bf",
   "metadata": {},
   "source": [
    "10. The primary difference is that the second block normalizes the features before applying gradient descent. Normalization can lead to faster convergence and more stable learning, especially when features have very different scales.\n",
    "11. The functions are redefined in both blocks to allow the second block to operate independently. This isn't necessary from a technical standpoint but can make it easier to experiment with different variations (like normalization) without modifying the original code.\n",
    "12. As we can see now the results are much much better and the predictions are really close to being accurate. \n",
    "This output demonstrates the progress of the gradient descent algorithm in optimizing the coefficients for a linear regression model.\n",
    "13. The cost function (which is based on Mean Squared Error) starts at a high value (615,969.52) and decreases significantly over the iterations. By iteration 900, the cost has reduced to 9.64, indicating that the model's predictions are closely aligning with the actual values.\n",
    "14. The prediction are very close to the actual y values. The closeness of these values indicates that the model is performing well.\n",
    "15. The final cost of 6.69 shows that the model has minimized the error between the predicted and actual values significantly. A lower cost value indicates better model performance.\n",
    "16. Overall, this output indicates a successful optimization of the linear regression model using gradient descent, resulting in accurate predictions and a low cos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a168f-de4c-4035-88db-7350c2b91745",
   "metadata": {},
   "source": [
    "### Problem 6. Do other cost functions work? (2 points)\n",
    "Repeat the process in problems 4 and 5 with MAE, and then again - with the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss). Both of them are less sensitive to outliers / anomalies than MSE); with the Huber loss function being specifically made for datasets with outliers.\n",
    "\n",
    "Explain your findings. Is there a cost function that works much better? How about speed of training (measured in wall time)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195ae12-2cb1-407c-95da-a8ffde4dd604",
   "metadata": {},
   "source": [
    "1. Let's first see what MAE(Mean Absolute Error) is. It is a commonly used metric to evaluate the accuracy of a regression model. It measures the average magnitude of the errors between the predicted values and the actual values, without considering their direction (i.e., whether the errors are positive or negative).\n",
    "2. Now we will basically repeat the same process like before but will be using MAE. The gradient descent process will berun using the MAE cost function. This updates the model coefficients iteratively based on the MAE gradients And after running gradient descent with MAE, the optimized coefficients are used to predict the target values, and these predictions are compared to the actual values.\n",
    "3. The code stops when the change in cost becomes very small or when the maximum number of iterations is reached. The final optimized coefficients and the cost history are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b3f0eb74-7b3d-4aa8-9749-dcb55e918a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae_and_gradients(X, y, coefficients):\n",
    "    \"\"\"\n",
    "    Compute the cost (MAE) and gradients for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        y (np.ndarray): The target values vector of shape (n,).\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        cost (float): The value of the cost function (MAE).\n",
    "        gradients (np.ndarray): The gradients of the cost function with respect to the coefficients\n",
    "                                of shape (m+1,).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]  # Number of observations\n",
    "    \n",
    "    # Add a bias column (column of ones) to X for the intercept term\n",
    "    X_with_bias = np.hstack((np.ones((n, 1)), X))\n",
    "    \n",
    "    # Compute the predictions\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    \n",
    "    # Compute the residuals\n",
    "    residuals = y_pred - y\n",
    "    \n",
    "    # Compute the cost (MAE)\n",
    "    cost = np.mean(np.abs(residuals))\n",
    "    \n",
    "    # Compute the gradients\n",
    "    gradients = np.sign(residuals).dot(X_with_bias) / n\n",
    "    \n",
    "    return cost, gradients\n",
    "\n",
    "def gradient_descent(X, y, initial_coefficients, compute_cost_and_gradients, learning_rate=0.01, iterations=1000, tolerance=1e-6):\n",
    "    coefficients = initial_coefficients.copy()\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cost, gradients = compute_cost_and_gradients(X, y, coefficients)\n",
    "        coefficients -= learning_rate * gradients\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n",
    "            print(f\"Stopping early at iteration {i} due to minimal change in cost.\")\n",
    "            break\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "\n",
    "    return coefficients, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d605c6fb-7ece-45a3-aa25-e4fe8f61416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 1093.565423145851\n",
      "Iteration 100: Cost 78.33877810718799\n",
      "Iteration 200: Cost 78.33877810718799\n",
      "Iteration 300: Cost 78.33877810718799\n",
      "Iteration 400: Cost 78.33877810718799\n",
      "Iteration 500: Cost 78.33877810718799\n",
      "Iteration 600: Cost 78.33877810718799\n",
      "Iteration 700: Cost 78.33877810718799\n",
      "Iteration 800: Cost 78.33877810718799\n",
      "Iteration 900: Cost 78.33877810718799\n",
      "First 5 Predictions using MAE: [ 825.12612339 1020.41495662  841.92370531 1022.46533953  810.33884972]\n",
      "First 5 Actual y: [ 888.43403034 1097.79411467  948.2163917  1091.09930987  898.3842639 ]\n",
      "Final Cost using MAE: 87.65150244269735\n"
     ]
    }
   ],
   "source": [
    "# Define initial coefficients (including the intercept)\n",
    "initial_coefficients = np.zeros(X.shape[1] + 1)\n",
    "\n",
    "# Run gradient descent using MAE\n",
    "optimized_coefficients_mae, cost_history_mae = gradient_descent(\n",
    "    X, y, initial_coefficients, compute_cost_and_gradients=compute_mae_and_gradients, learning_rate=0.01, iterations=1000\n",
    ")\n",
    "\n",
    "# Predict using the optimized coefficients\n",
    "y_pred_mae = linear_regression_predict(X, optimized_coefficients_mae)\n",
    "\n",
    "# Compare the prediction with the actual y values\n",
    "print(\"First 5 Predictions using MAE:\", y_pred_mae[:5])\n",
    "print(\"First 5 Actual y:\", y[:5])\n",
    "\n",
    "# Final cost\n",
    "print(f\"Final Cost using MAE: {cost_history_mae[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb4ed1-37df-430d-9ab9-bef3dfbce99b",
   "metadata": {},
   "source": [
    "4. After running it and getting the predicitons and final cost we can see the predictions are accurate but not as much as MSE.\n",
    "5. MSE performed better in terms of reducing the overall prediction error in the dataset. The final cost using MSE was significantly lower (6.69) compared to MAE (78.34). This indicates that the model trained with MSE was able to make predictions that were much closer to the actual values.\n",
    "6. MSE is highly sensitive to large errors due to the squaring of the residuals. This means that large discrepancies between predictions and actual values are penalized more heavily, leading to a stronger correction by the model. This property often results in a more accurate model for datasets where large errors need to be minimized.\n",
    "7. MAE did not perform as well as MSE in reducing the overall prediction error. The final cost with MAE was higher, indicating that the model had larger residuals (errors) compared to the model trained with MSE.\n",
    "8. MAE is less sensitive to large errors because it only takes the absolute value of the residuals, not their square. This makes MAE more robust to outliers, but it also means that it may not reduce large errors as effectively as MSE, leading to a higher overall cost.\n",
    "9. Both MSE and MAE used similar algorithms (gradient descent) for optimization, and hence the speed of training (measured in wall time) was comparable. However, the convergence behavior differs:\n",
    "MSE: The cost decreased steadily and significantly over iterations, suggesting that MSE drove faster convergence, particularly in the earlier stages of training.\n",
    "MAE: The cost plateaued more quickly, indicating that the model was making smaller adjustments. This could imply that MAE might converge faster in terms of iterations, but it might also get stuck in local minima, especially if there are large errors that are not being penalized enough.\n",
    "10. Now lets try with huber where we will again do the same functions and i will not explain all again we do the same as the previous ones\n",
    "11. Lets explain what huber is. Huber Loss is a combination of MSE and MAE. It behaves like MSE for small errors and like MAE for large errors. It is particularly useful for datasets with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9ef21dab-5337-405c-944e-532131b51e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_huber_and_gradients(X, y, coefficients, delta=1.0):\n",
    "    \"\"\"\n",
    "    Compute the cost (Huber loss) and gradients for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        y (np.ndarray): The target values vector of shape (n,).\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "        delta (float): The threshold at which the loss changes from MSE to MAE.\n",
    "                                   \n",
    "    Returns:\n",
    "        cost (float): The value of the cost function (Huber loss).\n",
    "        gradients (np.ndarray): The gradients of the cost function with respect to the coefficients\n",
    "                                of shape (m+1,).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]  # Number of observations\n",
    "    \n",
    "    # Add a bias column (column of ones) to X for the intercept term\n",
    "    X_with_bias = np.hstack((np.ones((n, 1)), X))\n",
    "    \n",
    "    # Compute the predictions\n",
    "    y_pred = np.dot(X_with_bias, coefficients)\n",
    "    \n",
    "    # Compute the residuals\n",
    "    residuals = y_pred - y\n",
    "    \n",
    "    # Compute the Huber loss\n",
    "    is_small_error = np.abs(residuals) <= delta\n",
    "    small_error_loss = 0.5 * residuals**2\n",
    "    large_error_loss = delta * (np.abs(residuals) - 0.5 * delta)\n",
    "    cost = np.where(is_small_error, small_error_loss, large_error_loss).mean()\n",
    "    \n",
    "    # Compute the gradients\n",
    "    gradients = np.where(is_small_error, residuals, delta * np.sign(residuals)).dot(X_with_bias) / n\n",
    "    \n",
    "    return cost, gradients\n",
    "\n",
    "def gradient_descent(X, y, initial_coefficients, compute_cost_and_gradients, learning_rate=0.01, iterations=1000, tolerance=1e-6):\n",
    "    coefficients = initial_coefficients.copy()\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cost, gradients = compute_cost_and_gradients(X, y, coefficients)\n",
    "        coefficients -= learning_rate * gradients\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n",
    "            print(f\"Stopping early at iteration {i} due to minimal change in cost.\")\n",
    "            break\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "\n",
    "    return coefficients, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bd1d445f-9906-4b5f-af27-bf3af5a581a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 1093.065423145851\n",
      "Iteration 100: Cost 87.48837003724209\n",
      "Iteration 200: Cost 87.48837003724209\n",
      "Iteration 300: Cost 87.48837003724209\n",
      "Iteration 400: Cost 87.48837003724209\n",
      "Iteration 500: Cost 87.48837003724209\n",
      "Iteration 600: Cost 87.48837003724209\n",
      "Iteration 700: Cost 87.48837003724209\n",
      "Iteration 800: Cost 87.48837003724209\n",
      "Iteration 900: Cost 87.48837003724209\n",
      "First 5 Predictions using Huber Loss: [ 971.65056013 1186.10435149  967.48910102 1184.3489254   943.41202437]\n",
      "First 5 Actual y: [ 888.43403034 1097.79411467  948.2163917  1091.09930987  898.3842639 ]\n",
      "Final Cost using Huber Loss: 77.50191051264319\n"
     ]
    }
   ],
   "source": [
    "# Run gradient descent using Huber Loss\n",
    "optimized_coefficients_huber, cost_history_huber = gradient_descent(\n",
    "    X, y, initial_coefficients, compute_cost_and_gradients=lambda X, y, coeffs: compute_huber_and_gradients(X, y, coeffs, delta=1.0), learning_rate=0.01, iterations=1000\n",
    ")\n",
    "\n",
    "# Predict using the optimized coefficients\n",
    "y_pred_huber = linear_regression_predict(X, optimized_coefficients_huber)\n",
    "\n",
    "# Compare the prediction with the actual y values\n",
    "print(\"First 5 Predictions using Huber Loss:\", y_pred_huber[:5])\n",
    "print(\"First 5 Actual y:\", y[:5])\n",
    "\n",
    "# Final cost\n",
    "print(f\"Final Cost using Huber Loss: {cost_history_huber[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f26557-dee7-4566-a53e-1143c43fe3ea",
   "metadata": {},
   "source": [
    "12. The final cost using Huber Loss was 77.50, which is lower than the final cost of MAE (87.65) but higher than that of MSE (6.69). This indicates that Huber Loss performed better than MAE in reducing the prediction errors but was not as effective as MSE in this particular case.\n",
    "13. The first 5 predictions using Huber Loss were closer to the actual values compared to MAE but not as close as those produced by MSE. This suggests that Huber Loss is better at handling outliers than MAE while still providing reasonably accurate predictions.\n",
    "14. MSE remains the best choice for this dataset in terms of minimizing the overall error and achieving the most accurate predictions. It is especially effective when the dataset does not have significant outliers and when reducing large errors is critical.\n",
    "15. Huber Loss provides a good balance between MSE and MAE. It is more robust to outliers than MSE and more effective at reducing errors than MAE. This makes it a good choice when you want a compromise between accuracy and robustness.\n",
    "16. MAE is the least effective for this dataset in terms of accuracy but is the most robust to outliers. It should be considered when outliers are a significant concern, and reducing their influence is more important than achieving the lowest possible error.\n",
    "17. In summary, Huber Loss offers a balanced approach and performs better than MAE but not as well as MSE in terms of accuracy. It is a suitable choice if your dataset has some outliers and you want to achieve a good trade-off between robustness and precision. However, if accuracy is your primary concern and outliers are not significant, MSE is the better option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4176d1-6cad-4830-824c-1d27e50efe89",
   "metadata": {},
   "source": [
    "### Problem 7. Experiment with the learning rate (1 point)\n",
    "Use your favorite cost function. Run several \"experiments\" with different learning rates. Try really small, and really large values. Observe and document your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3d1c3-e59b-4c84-94c6-5645c8edaa48",
   "metadata": {},
   "source": [
    "1. We will use the MSE as we have seen its the most effective for our dataset.\n",
    "2. Let's create a function to perform gradient descent and make a for loop in which we will go through different learning rates and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "71f04525-95f3-427d-8763-27af6839cb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with learning rate: 1e-06\n",
      "Iteration 0: Cost 615969.5230157449\n",
      "Iteration 100: Cost 24334.13151915987\n",
      "Iteration 200: Cost 5108.792812472266\n",
      "Iteration 300: Cost 4248.038514084159\n",
      "Iteration 400: Cost 3994.561665640788\n",
      "Iteration 500: Cost 3777.4230089393864\n",
      "Iteration 600: Cost 3576.680673462712\n",
      "Iteration 700: Cost 3390.2521978440286\n",
      "Iteration 800: Cost 3216.8003003442113\n",
      "Iteration 900: Cost 3055.1599935318873\n",
      "Final Cost for learning rate 1e-06: 2905.75798427696\n",
      "Optimized Coefficients for learning rate 1e-06: [0.06682525 2.53541    0.60149723 1.27128487 3.20049017]\n",
      "\n",
      "Testing with learning rate: 0.0001\n",
      "Iteration 0: Cost 615969.5230157449\n",
      "Iteration 100: Cost 312.44214904502496\n",
      "Iteration 200: Cost 184.4013911064608\n",
      "Iteration 300: Cost 118.1304852674021\n",
      "Iteration 400: Cost 76.22194109103613\n",
      "Iteration 500: Cost 49.47620788030608\n",
      "Iteration 600: Cost 32.339097871809756\n",
      "Iteration 700: Cost 21.32418435862668\n",
      "Iteration 800: Cost 14.225197581831477\n",
      "Iteration 900: Cost 9.639098337944509\n",
      "Final Cost for learning rate 0.0001: 6.693865720669176\n",
      "Optimized Coefficients for learning rate 0.0001: [0.10912229 5.04821357 1.75903711 1.08778503 3.01242783]\n",
      "\n",
      "Testing with learning rate: 0.01\n",
      "Iteration 0: Cost 615969.5230157449\n",
      "Iteration 100: Cost inf\n",
      "Iteration 200: Cost nan\n",
      "Iteration 300: Cost nan\n",
      "Iteration 400: Cost nan\n",
      "Iteration 500: Cost nan\n",
      "Iteration 600: Cost nan\n",
      "Iteration 700: Cost nan\n",
      "Iteration 800: Cost nan\n",
      "Iteration 900: Cost nan\n",
      "Final Cost for learning rate 0.01: nan\n",
      "Optimized Coefficients for learning rate 0.01: [nan nan nan nan nan]\n",
      "\n",
      "Testing with learning rate: 0.1\n",
      "Iteration 0: Cost 615969.5230157449\n",
      "Iteration 100: Cost nan\n",
      "Iteration 200: Cost nan\n",
      "Iteration 300: Cost nan\n",
      "Iteration 400: Cost nan\n",
      "Iteration 500: Cost nan\n",
      "Iteration 600: Cost nan\n",
      "Iteration 700: Cost nan\n",
      "Iteration 800: Cost nan\n",
      "Iteration 900: Cost nan\n",
      "Final Cost for learning rate 0.1: nan\n",
      "Optimized Coefficients for learning rate 0.1: [nan nan nan nan nan]\n",
      "\n",
      "Testing with learning rate: 1\n",
      "Iteration 0: Cost 615969.5230157449\n",
      "Iteration 100: Cost nan\n",
      "Iteration 200: Cost nan\n",
      "Iteration 300: Cost nan\n",
      "Iteration 400: Cost nan\n",
      "Iteration 500: Cost nan\n",
      "Iteration 600: Cost nan\n",
      "Iteration 700: Cost nan\n",
      "Iteration 800: Cost nan\n",
      "Iteration 900: Cost nan\n",
      "Final Cost for learning rate 1: nan\n",
      "Optimized Coefficients for learning rate 1: [nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_5756\\443740202.py:23: RuntimeWarning: overflow encountered in square\n",
      "  cost = (1 / (2 * n)) * np.sum(residuals**2)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_5756\\2709884112.py:10: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, initial_coefficients, compute_cost_and_gradients, learning_rate=0.01, iterations=1000, tolerance=1e-6):\n",
    "    coefficients = initial_coefficients.copy()\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cost, gradients = compute_cost_and_gradients(X, y, coefficients)\n",
    "        coefficients -= learning_rate * gradients\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n",
    "            print(f\"Stopping early at iteration {i} due to minimal change in cost.\")\n",
    "            break\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "\n",
    "    return coefficients, cost_history\n",
    "\n",
    "# Different learning rates to experiment with\n",
    "learning_rates = [1e-6, 1e-4, 0.01, 0.1, 1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting with learning rate: {lr}\")\n",
    "    \n",
    "    # Re-initialize coefficients\n",
    "    initial_coefficients = np.zeros(X.shape[1] + 1)\n",
    "    \n",
    "    # Run gradient descent\n",
    "    optimized_coefficients, cost_history = gradient_descent(X, y, initial_coefficients, compute_cost_and_gradients, learning_rate=lr, iterations=1000)\n",
    "    \n",
    "    # Final Cost\n",
    "    print(f\"Final Cost for learning rate {lr}: {cost_history[-1]}\")\n",
    "    print(f\"Optimized Coefficients for learning rate {lr}: {optimized_coefficients[:5]}\")  # Print the first 5 coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bc91b-c37c-4056-8c15-39823edb6ea8",
   "metadata": {},
   "source": [
    "3. The errors we encountered during our experiments with different learning rates are typical in gradient descent algorithms and are primarily due to the behavior of the algorithm when the learning rate is either too large or too small.\n",
    "4. Based on the results of your experiments, the learning rate of 0.0001 appears to be the most effective for this specific problem\n",
    "5. Learning Rate 1e-6: Final Cost: 2905.75798427696. The cost decreases steadily but very slowly. The final cost is relatively high compared to other learning rates. This indicates that the learning rate is too small, causing the gradient descent to converge very slowly.\n",
    "6. Learning Rate 0.0001: Final Cost: 6.693865720669176. The cost decreases effectively and converges to a low value. This learning rate provides a good balance between speed and convergence, leading to accurate results without overshooting or diverging.\n",
    "7. Learning Rate 0.01: Final Cost: nan. The gradient descent algorithm encounters issues with overflow and invalid operations (resulting in NaN values). This indicates that the learning rate is too large, causing the algorithm to overshoot the minimum and diverge.\n",
    "8. Learning Rate 0.1 and 1: Final Cost: nan. Similar to the learning rate of 0.01, these learning rates are far too large, causing the algorithm to diverge immediately. The cost function becomes undefined (nan), and the coefficients are not optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91385302-2fc1-48a4-a453-cce7f038e9c2",
   "metadata": {},
   "source": [
    "### Problem 8. Generate some data for classification (1 point)\n",
    "You'll need to create two clusters of points (one cluster for each class). I recomment using `scikit-learn`'s `make_blobs()` ([info](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)). Use as many features as you used in problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b948009d-6af6-468c-ac6e-bacd4f75498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000  # Number of samples\n",
    "n_features = 10   # Number of features (same as in Problem 1)\n",
    "n_classes = 2     # Number of classes (binary classification)\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, \n",
    "                  n_features=n_features, \n",
    "                  centers=n_classes, \n",
    "                  random_state=50)\n",
    "\n",
    "# Step 2: Convert to a DataFrame\n",
    "columns = [f'x{i+1}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=columns)\n",
    "df['class'] = y\n",
    "\n",
    "\n",
    "# Step 4: Save the data to a CSV file\n",
    "df.to_csv('classification_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6246890-0808-40e1-b892-fa1d11fd088c",
   "metadata": {},
   "source": [
    "### Problem 9. Perform logistic regression (1 point)\n",
    "Reuse the code you wrote in problems 3-7 as much as possible. If you wrote vectorized functions with variable parameters - you should find this easy. If not - it's not too late to go back and refactor your code.\n",
    "\n",
    "The modelling function for logistic regression is\n",
    "$$ \\tilde{y} = \\frac{1}{1+\\exp{(-\\sum_{i=1}^{m}a_i x_i + b)}}$$. Find a way to represent it using as much of your previous code as you can.\n",
    "\n",
    "The most commonly used loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy).\n",
    "\n",
    "Experiment with different learning rates, basically repeating what you did in problem 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "107e100b-1627-4300-8d2e-98e8b438e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_predict(X, coefficients):\n",
    "    \"\"\"\n",
    "    Predict the output probabilities using a logistic regression model with the intercept represented as a coefficient.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        np.ndarray: The predicted probability vector of shape (n,).\n",
    "    \"\"\"\n",
    "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    linear_combination = np.dot(X_with_bias, coefficients)\n",
    "    y_pred = 1 / (1 + np.exp(-linear_combination))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1d6b46d6-ae4b-4c8a-ba0e-50231af05fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_and_gradients(X, y, coefficients):\n",
    "    \"\"\"\n",
    "    Compute the cost (cross-entropy) and gradients for logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data matrix (n x m), where n is the number of observations\n",
    "                        and m is the number of features.\n",
    "        y (np.ndarray): The target values vector of shape (n,).\n",
    "        coefficients (np.ndarray): The coefficients array of shape (m+1,), where the first\n",
    "                                   element represents the intercept term.\n",
    "                                   \n",
    "    Returns:\n",
    "        cost (float): The value of the cost function (cross-entropy).\n",
    "        gradients (np.ndarray): The gradients of the cost function with respect to the coefficients\n",
    "                                of shape (m+1,).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    y_pred = logistic_regression_predict(X, coefficients)\n",
    "    \n",
    "    # Compute the cross-entropy cost\n",
    "    cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    \n",
    "    # Compute the gradients\n",
    "    X_with_bias = np.hstack((np.ones((n, 1)), X))\n",
    "    residuals = y_pred - y\n",
    "    gradients = np.dot(X_with_bias.T, residuals) / n\n",
    "    \n",
    "    return cost, gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8d245635-0561-4c64-9ea3-12bb445734c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, initial_coefficients, compute_cost_and_gradients, learning_rate=0.01, iterations=1000, tolerance=1e-6):\n",
    "    coefficients = initial_coefficients.copy()\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cost, gradients = compute_cost_and_gradients(X, y, coefficients)\n",
    "        coefficients -= learning_rate * gradients\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n",
    "            print(f\"Stopping early at iteration {i} due to minimal change in cost.\")\n",
    "            break\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "\n",
    "    return coefficients, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a836ad13-2a1b-45cc-bfcc-f61f5ec02f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with learning rate: 1e-06\n",
      "Iteration 0: Cost 0.6931471805599454\n",
      "Iteration 100: Cost 0.6909189781962258\n",
      "Iteration 200: Cost 0.6887013457138935\n",
      "Iteration 300: Cost 0.6864942279055062\n",
      "Iteration 400: Cost 0.6842975697623824\n",
      "Iteration 500: Cost 0.6821113164765378\n",
      "Iteration 600: Cost 0.6799354134425512\n",
      "Iteration 700: Cost 0.6777698062593617\n",
      "Iteration 800: Cost 0.6756144407319964\n",
      "Iteration 900: Cost 0.6734692628732329\n",
      "Final Cost for learning rate 1e-06: 0.6713555193581787\n",
      "Optimized Coefficients for learning rate 1e-06: [-1.26434100e-06 -7.43980223e-04  6.16099115e-04 -5.37112510e-04\n",
      "  2.84985298e-03]\n",
      "\n",
      "Testing with learning rate: 0.0001\n",
      "Iteration 0: Cost 0.6931471805599454\n",
      "Iteration 100: Cost 0.5144210108312051\n",
      "Iteration 200: Cost 0.3992284441956458\n",
      "Iteration 300: Cost 0.3214667083104801\n",
      "Iteration 400: Cost 0.26662828047017206\n",
      "Iteration 500: Cost 0.22645033911926787\n",
      "Iteration 600: Cost 0.19603795694339288\n",
      "Iteration 700: Cost 0.17237248502433003\n",
      "Iteration 800: Cost 0.15352068649024564\n",
      "Iteration 900: Cost 0.1382010988396649\n",
      "Final Cost for learning rate 0.0001: 0.12565333056804553\n",
      "Optimized Coefficients for learning rate 0.0001: [-0.00187517 -0.03213441  0.03652687 -0.01448214  0.12743972]\n",
      "\n",
      "Testing with learning rate: 0.01\n",
      "Iteration 0: Cost 0.6931471805599454\n",
      "Iteration 100: Cost 0.01267142488926308\n",
      "Iteration 200: Cost 0.006409461406656208\n",
      "Iteration 300: Cost 0.0043048305234744235\n",
      "Iteration 400: Cost 0.0032467389269713624\n",
      "Iteration 500: Cost 0.00260915295662671\n",
      "Iteration 600: Cost 0.0021825723632223895\n",
      "Iteration 700: Cost 0.0018769310573936362\n",
      "Iteration 800: Cost 0.0016470764983721453\n",
      "Iteration 900: Cost 0.0014678654000796108\n",
      "Final Cost for learning rate 0.01: 0.0013254760245597443\n",
      "Optimized Coefficients for learning rate 0.01: [-0.00841537 -0.10660593  0.13248076 -0.03911629  0.42702181]\n",
      "\n",
      "Testing with learning rate: 0.1\n",
      "Iteration 0: Cost 0.6931471805599454\n",
      "Iteration 100: Cost 0.0012041279379724058\n",
      "Iteration 200: Cost 0.000639990202609491\n",
      "Iteration 300: Cost 0.00043771686726402724\n",
      "Stopping early at iteration 355 due to minimal change in cost.\n",
      "Final Cost for learning rate 0.1: 0.0003732944372796552\n",
      "Optimized Coefficients for learning rate 0.1: [-0.01018945 -0.12807332  0.15980698 -0.04636602  0.51254331]\n",
      "\n",
      "Testing with learning rate: 1\n",
      "Iteration 0: Cost 0.6931471805599454\n",
      "Iteration 100: Cost nan\n",
      "Iteration 200: Cost nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_5756\\432213947.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_5756\\432213947.py:21: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300: Cost nan\n",
      "Iteration 400: Cost nan\n",
      "Iteration 500: Cost nan\n",
      "Iteration 600: Cost nan\n",
      "Iteration 700: Cost nan\n",
      "Iteration 800: Cost nan\n",
      "Iteration 900: Cost nan\n",
      "Final Cost for learning rate 1: nan\n",
      "Optimized Coefficients for learning rate 1: [-1.61702152e-09 -7.54429298e-01  6.17745429e-01 -5.50866191e-01\n",
      "  2.88684164e+00]\n"
     ]
    }
   ],
   "source": [
    "# Initialize coefficients (including the intercept)\n",
    "initial_coefficients = np.zeros(X.shape[1] + 1)\n",
    "\n",
    "# Different learning rates to experiment with\n",
    "learning_rates = [1e-6, 1e-4, 0.01, 0.1, 1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting with learning rate: {lr}\")\n",
    "    \n",
    "    # Run gradient descent\n",
    "    optimized_coefficients, cost_history = gradient_descent(X, y, initial_coefficients, compute_cross_entropy_and_gradients, learning_rate=lr, iterations=1000)\n",
    "    \n",
    "    # Final Cost\n",
    "    print(f\"Final Cost for learning rate {lr}: {cost_history[-1]}\")\n",
    "    print(f\"Optimized Coefficients for learning rate {lr}: {optimized_coefficients[:5]}\")  # Print the first 5 coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411df95f-5f16-4c37-8649-2c495eb96974",
   "metadata": {},
   "source": [
    "### * Problem 10. Continue experimenting and delving deep into ML\n",
    "You just saw how modelling works and how to implement some code. Some of the things you can think about (and I recommend you pause and ponder on some of them are):\n",
    "* Code: OOP can be your friend sometimes. `scikit-learn`'s models have `fit()`, `predict()` and `score()` methods.\n",
    "* Data: What approaches work on non-generated data?\n",
    "* Evaluation: How well do different models (and their \"settings\" - hyperparameters) actually work in practice? How do we evaluate a model in a meaningful way?\n",
    "* Optimization - maths: Look at what `optimizers` (or solvers) are used in `scikit-learn` and why. Many \"tricks\" revolve around making the algorithm converge (finish) in fewer iterations, or making it more numerically stable.\n",
    "* Optimization - code: Are there ways to make the code run fastr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08101fb6-d53f-4dc9-8f37-d07478c9e220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
