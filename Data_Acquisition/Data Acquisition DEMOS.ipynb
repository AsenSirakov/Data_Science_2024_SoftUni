{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9794e4b-2535-4891-ad57-04543574ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c1725-8d45-44d6-a6c2-634b48727794",
   "metadata": {},
   "source": [
    "# Data Acquisiton DEMOS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48295f16-127f-41b8-b828-b7228b5bb564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/96.5 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 41.0/96.5 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 96.5/96.5 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb442ee-7650-4853-9596-1ce686c08882",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = pd.read_csv(\"Data/accidents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397313d5-d15a-4224-988f-296178c2a1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Miles from Home</th>\n",
       "      <th>% of Accidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>less than 1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 to 5</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6 to 10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11 to 15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16 to 20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>over 20</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Miles from Home  % of Accidents\n",
       "0     less than 1              23\n",
       "1          1 to 5              29\n",
       "2         6 to 10              17\n",
       "3        11 to 15               8\n",
       "4        16 to 20               6\n",
       "5         over 20              17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d93381c-8620-470c-af10-316814f15aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Green = pd.read_excel(\"Data/green_tripdata_2015-09.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea1fded7-427d-4ab1-827e-78e20057f002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'lpep_pickup_datetime', 'Lpep_dropoff_datetime',\n",
       "       'Store_and_fwd_flag', 'RateCodeID', 'Pickup_longitude',\n",
       "       'Pickup_latitude', 'Dropoff_longitude', 'Dropoff_latitude',\n",
       "       'Passenger_count', 'Trip_distance', 'Fare_amount', 'Extra', 'MTA_tax',\n",
       "       'Tip_amount', 'Tolls_amount', 'Ehail_fee', 'improvement_surcharge',\n",
       "       'Total_amount', 'Payment_type', 'Trip_type '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe2d82c3-63af-46ca-a514-f6fcdbafc6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2\n",
       "1        2\n",
       "2        2\n",
       "3        2\n",
       "4        2\n",
       "        ..\n",
       "65530    2\n",
       "65531    2\n",
       "65532    2\n",
       "65533    2\n",
       "65534    2\n",
       "Name: VendorID, Length: 65535, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Green.VendorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a28d40c-3978-4e42-92a4-fb2719d4941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vendor_ids = Green['VendorID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f377249-4609-4ec5-a75d-37f9ad5e5a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "print(unique_vendor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f8c419b-2acb-4660-87f1-985d1b886a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_counts = Green['VendorID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2caa0ff-9414-48be-818b-c1526e7eeb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID\n",
       "2    51412\n",
       "1    14123\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vendor_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff5a4c0e-f094-4822-b846-8feaeacda872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>Lpep_dropoff_datetime</th>\n",
       "      <th>Store_and_fwd_flag</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>Pickup_longitude</th>\n",
       "      <th>Pickup_latitude</th>\n",
       "      <th>Dropoff_longitude</th>\n",
       "      <th>Dropoff_latitude</th>\n",
       "      <th>Passenger_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Fare_amount</th>\n",
       "      <th>Extra</th>\n",
       "      <th>MTA_tax</th>\n",
       "      <th>Tip_amount</th>\n",
       "      <th>Tolls_amount</th>\n",
       "      <th>Ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>Total_amount</th>\n",
       "      <th>Payment_type</th>\n",
       "      <th>Trip_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-01 00:02:34</td>\n",
       "      <td>2015-09-01 00:02:38</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.979485</td>\n",
       "      <td>40.684956</td>\n",
       "      <td>-73.979431</td>\n",
       "      <td>40.685020</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.75</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-01 00:04:20</td>\n",
       "      <td>2015-09-01 00:04:24</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>-74.010796</td>\n",
       "      <td>40.912216</td>\n",
       "      <td>-74.010780</td>\n",
       "      <td>40.912212</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-01 00:01:50</td>\n",
       "      <td>2015-09-01 00:04:24</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.921410</td>\n",
       "      <td>40.766708</td>\n",
       "      <td>-73.914413</td>\n",
       "      <td>40.764687</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-01 00:02:36</td>\n",
       "      <td>2015-09-01 00:06:42</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.921387</td>\n",
       "      <td>40.766678</td>\n",
       "      <td>-73.931427</td>\n",
       "      <td>40.771584</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-01 00:00:14</td>\n",
       "      <td>2015-09-01 00:04:20</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.955482</td>\n",
       "      <td>40.714046</td>\n",
       "      <td>-73.944412</td>\n",
       "      <td>40.714729</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65530</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-02 16:51:59</td>\n",
       "      <td>2015-09-02 17:04:00</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.829605</td>\n",
       "      <td>40.759716</td>\n",
       "      <td>-73.832214</td>\n",
       "      <td>40.751514</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65531</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-02 16:53:51</td>\n",
       "      <td>2015-09-02 17:04:32</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.962112</td>\n",
       "      <td>40.805710</td>\n",
       "      <td>-73.984970</td>\n",
       "      <td>40.769550</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65532</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-02 16:57:21</td>\n",
       "      <td>2015-09-02 17:05:03</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.829941</td>\n",
       "      <td>40.713718</td>\n",
       "      <td>-73.831917</td>\n",
       "      <td>40.702145</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65533</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-02 16:51:42</td>\n",
       "      <td>2015-09-02 17:05:28</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.860748</td>\n",
       "      <td>40.832661</td>\n",
       "      <td>-73.845169</td>\n",
       "      <td>40.845306</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65534</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-02 16:40:36</td>\n",
       "      <td>2015-09-02 16:48:20</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.941887</td>\n",
       "      <td>40.822948</td>\n",
       "      <td>-73.937180</td>\n",
       "      <td>40.804462</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65535 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VendorID lpep_pickup_datetime Lpep_dropoff_datetime Store_and_fwd_flag  \\\n",
       "0             2  2015-09-01 00:02:34   2015-09-01 00:02:38                  N   \n",
       "1             2  2015-09-01 00:04:20   2015-09-01 00:04:24                  N   \n",
       "2             2  2015-09-01 00:01:50   2015-09-01 00:04:24                  N   \n",
       "3             2  2015-09-01 00:02:36   2015-09-01 00:06:42                  N   \n",
       "4             2  2015-09-01 00:00:14   2015-09-01 00:04:20                  N   \n",
       "...         ...                  ...                   ...                ...   \n",
       "65530         2  2015-09-02 16:51:59   2015-09-02 17:04:00                  N   \n",
       "65531         2  2015-09-02 16:53:51   2015-09-02 17:04:32                  N   \n",
       "65532         2  2015-09-02 16:57:21   2015-09-02 17:05:03                  N   \n",
       "65533         2  2015-09-02 16:51:42   2015-09-02 17:05:28                  N   \n",
       "65534         2  2015-09-02 16:40:36   2015-09-02 16:48:20                  N   \n",
       "\n",
       "       RateCodeID  Pickup_longitude  Pickup_latitude  Dropoff_longitude  \\\n",
       "0               5        -73.979485        40.684956         -73.979431   \n",
       "1               5        -74.010796        40.912216         -74.010780   \n",
       "2               1        -73.921410        40.766708         -73.914413   \n",
       "3               1        -73.921387        40.766678         -73.931427   \n",
       "4               1        -73.955482        40.714046         -73.944412   \n",
       "...           ...               ...              ...                ...   \n",
       "65530           1        -73.829605        40.759716         -73.832214   \n",
       "65531           1        -73.962112        40.805710         -73.984970   \n",
       "65532           1        -73.829941        40.713718         -73.831917   \n",
       "65533           1        -73.860748        40.832661         -73.845169   \n",
       "65534           1        -73.941887        40.822948         -73.937180   \n",
       "\n",
       "       Dropoff_latitude  Passenger_count  ...  Fare_amount  Extra  MTA_tax  \\\n",
       "0             40.685020                1  ...          7.8    0.0      0.0   \n",
       "1             40.912212                1  ...         45.0    0.0      0.0   \n",
       "2             40.764687                1  ...          4.0    0.5      0.5   \n",
       "3             40.771584                1  ...          5.0    0.5      0.5   \n",
       "4             40.714729                1  ...          5.0    0.5      0.5   \n",
       "...                 ...              ...  ...          ...    ...      ...   \n",
       "65530         40.751514                1  ...          9.0    1.0      0.5   \n",
       "65531         40.769550                1  ...         10.5    1.0      0.5   \n",
       "65532         40.702145                1  ...          7.0    1.0      0.5   \n",
       "65533         40.845306                1  ...         10.5    1.0      0.5   \n",
       "65534         40.804462                1  ...          8.0    1.0      0.5   \n",
       "\n",
       "       Tip_amount  Tolls_amount  Ehail_fee  improvement_surcharge  \\\n",
       "0            1.95           0.0        NaN                    0.0   \n",
       "1            0.00           0.0        NaN                    0.0   \n",
       "2            0.50           0.0        NaN                    0.3   \n",
       "3            0.00           0.0        NaN                    0.3   \n",
       "4            0.00           0.0        NaN                    0.3   \n",
       "...           ...           ...        ...                    ...   \n",
       "65530        0.00           0.0        NaN                    0.3   \n",
       "65531        2.46           0.0        NaN                    0.3   \n",
       "65532        2.20           0.0        NaN                    0.3   \n",
       "65533        2.46           0.0        NaN                    0.3   \n",
       "65534        0.00           0.0        NaN                    0.3   \n",
       "\n",
       "       Total_amount  Payment_type  Trip_type   \n",
       "0              9.75             1           2  \n",
       "1             45.00             1           2  \n",
       "2              5.80             1           1  \n",
       "3              6.30             2           1  \n",
       "4              6.30             2           1  \n",
       "...             ...           ...         ...  \n",
       "65530         10.80             2           1  \n",
       "65531         14.76             1           1  \n",
       "65532         11.00             1           1  \n",
       "65533         14.76             1           1  \n",
       "65534          9.80             2           1  \n",
       "\n",
       "[65535 rows x 21 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c036428b-1b97-4cce-9e5e-70a761ccd9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Y'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Green['Store_and_fwd_flag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e56af1-7f66-4c6a-a821-e29d80f1b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "Green['Store_and_fwd_flag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c94ce36-05cc-4aee-b51f-e0f901a7f281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store_and_fwd_flag\n",
       "N    65181\n",
       "Y      354\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Green['Store_and_fwd_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd9660f4-2b14-4e09-afdb-2e1aee636863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <td>Never forget what you are. The rest of the wor...</td>\n",
       "      <td>You love your children. It's your one redeemin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character</th>\n",
       "      <td>{'name': 'Tyrion Lannister', 'slug': 'tyrion',...</td>\n",
       "      <td>{'name': 'Tyrion Lannister', 'slug': 'tyrion',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0  \\\n",
       "sentence   Never forget what you are. The rest of the wor...   \n",
       "character  {'name': 'Tyrion Lannister', 'slug': 'tyrion',...   \n",
       "\n",
       "                                                           1  \n",
       "sentence   You love your children. It's your one redeemin...  \n",
       "character  {'name': 'Tyrion Lannister', 'slug': 'tyrion',...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"https://api.gameofthronesquotes.xyz/v1/author/tyrion/2\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e74ee5c-7b5a-45ea-b3c8-16cc58f722bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.0.25)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a75c4db7-80fe-41c9-8d51-c4e0c6df7d72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 40, saw 21\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/plotly/datasets/blob/master/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 40, saw 21\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv(\"https://github.com/plotly/datasets/blob/master/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f9aa8b-bf2e-43dc-8a69-e63f93d195ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/Beginner_Reviews_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5ed449-b9db-48b8-8493-5974ef0d2d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " Unnamed: 0    0\n",
      "sentence      0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e8aef3-0c63-4fc2-b435-339cad5d8b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "duplicates = data.duplicated().sum()\n",
    "print(\"Duplicates:\", duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e397049f-1a57-4382-99b6-31ffa980662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "         Unnamed: 0       label\n",
      "count  1000.000000  1000.00000\n",
      "mean    499.500000     0.50000\n",
      "std     288.819436     0.50025\n",
      "min       0.000000     0.00000\n",
      "25%     249.750000     0.00000\n",
      "50%     499.500000     0.50000\n",
      "75%     749.250000     1.00000\n",
      "max     999.000000     1.00000\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary Statistics:\\n\", data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63b28816-d673-401b-a207-bae4e4701901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18852\\3205229357.py:49: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 10, 'clf__solver': 'liblinear', 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Best Cross-Validation Accuracy: 0.8\n",
      "Accuracy of Best Model: 0.8\n",
      "Classification Report of Best Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        96\n",
      "           1       0.83      0.77      0.80       104\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.80      0.80      0.80       200\n",
      "weighted avg       0.80      0.80      0.80       200\n",
      "\n",
      "                                              sentence  predicted_label  \\\n",
      "521                   If you haven't gone here GO NOW!                1   \n",
      "737  Try them in the airport to experience some tas...                1   \n",
      "740  The restaurant is very clean and has a family ...                1   \n",
      "660  I personally love the hummus, pita, baklava, f...                1   \n",
      "411              Come hungry, leave happy and stuffed!                1   \n",
      "678      It's a great place and I highly recommend it.                1   \n",
      "626  Best of luck to the rude and non-customer serv...                0   \n",
      "513                            Reasonably priced also!                1   \n",
      "859            Worst food/service I've had in a while.                0   \n",
      "136            I had a seriously solid breakfast here.                1   \n",
      "\n",
      "     actual_label  \n",
      "521             1  \n",
      "737             1  \n",
      "740             1  \n",
      "660             1  \n",
      "411             1  \n",
      "678             1  \n",
      "626             0  \n",
      "513             1  \n",
      "859             0  \n",
      "136             1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Data/Beginner_Reviews_dataset.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Check for missing values and drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add negation words to a separate set\n",
    "negation_words = set([\"not\", \"no\", \"never\", \"none\", \"n't\"])\n",
    "\n",
    "# Function to expand contractions\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "    \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        text = re.sub(contraction, expanded, text)\n",
    "    return text\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Handle negations\n",
    "    processed_words = []\n",
    "    skip_next = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word in negation_words and i + 1 < len(words):\n",
    "            processed_words.append(f\"{word}_{words[i + 1]}\")\n",
    "            skip_next = True\n",
    "        elif skip_next:\n",
    "            skip_next = False\n",
    "        elif word not in stop_words:\n",
    "            processed_words.append(lemmatizer.lemmatize(word))\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "data['cleaned_sentence'] = data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_cleaned = data['cleaned_sentence']\n",
    "y_cleaned = data['label']\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validation Accuracy: {best_score}')\n",
    "\n",
    "# Predict with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_cleaned)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best = accuracy_score(y_test_cleaned, y_pred_best)\n",
    "report_best = classification_report(y_test_cleaned, y_pred_best)\n",
    "\n",
    "print(f'Accuracy of Best Model: {accuracy_best}')\n",
    "print(f'Classification Report of Best Model: \\n{report_best}')\n",
    "\n",
    "# Create a DataFrame to display the sentences, their actual labels, and the predicted labels\n",
    "test_data_with_predictions = pd.DataFrame({\n",
    "    'sentence': data.loc[y_test_cleaned.index, 'sentence'],  # Original sentences\n",
    "    'predicted_label': y_pred_best,\n",
    "    'actual_label': y_test_cleaned\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(test_data_with_predictions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43c909f-00bc-4c1b-b315-eda6a8b44804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18852\\2526321193.py:49: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 10, 'clf__kernel': 'rbf', 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Best Cross-Validation Accuracy: 0.8125\n",
      "Accuracy of Best Model: 0.815\n",
      "Classification Report of Best Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81        96\n",
      "           1       0.85      0.79      0.82       104\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.82      0.82      0.81       200\n",
      "weighted avg       0.82      0.81      0.82       200\n",
      "\n",
      "                                              sentence  predicted_label  \\\n",
      "521                   If you haven't gone here GO NOW!                1   \n",
      "737  Try them in the airport to experience some tas...                1   \n",
      "740  The restaurant is very clean and has a family ...                1   \n",
      "660  I personally love the hummus, pita, baklava, f...                1   \n",
      "411              Come hungry, leave happy and stuffed!                1   \n",
      "678      It's a great place and I highly recommend it.                1   \n",
      "626  Best of luck to the rude and non-customer serv...                0   \n",
      "513                            Reasonably priced also!                1   \n",
      "859            Worst food/service I've had in a while.                0   \n",
      "136            I had a seriously solid breakfast here.                1   \n",
      "\n",
      "     actual_label  \n",
      "521             1  \n",
      "737             1  \n",
      "740             1  \n",
      "660             1  \n",
      "411             1  \n",
      "678             1  \n",
      "626             0  \n",
      "513             1  \n",
      "859             0  \n",
      "136             1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Data/Beginner_Reviews_dataset.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Check for missing values and drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add negation words to a separate set\n",
    "negation_words = set([\"not\", \"no\", \"never\", \"none\", \"n't\"])\n",
    "\n",
    "# Function to expand contractions\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "    \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        text = re.sub(contraction, expanded, text)\n",
    "    return text\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Handle negations\n",
    "    processed_words = []\n",
    "    skip_next = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word in negation_words and i + 1 < len(words):\n",
    "            processed_words.append(f\"{word}_{words[i + 1]}\")\n",
    "            skip_next = True\n",
    "        elif skip_next:\n",
    "            skip_next = False\n",
    "        elif word not in stop_words:\n",
    "            processed_words.append(lemmatizer.lemmatize(word))\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "data['cleaned_sentence'] = data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_cleaned = data['cleaned_sentence']\n",
    "y_cleaned = data['label']\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and SVM classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validation Accuracy: {best_score}')\n",
    "\n",
    "# Predict with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_cleaned)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best = accuracy_score(y_test_cleaned, y_pred_best)\n",
    "report_best = classification_report(y_test_cleaned, y_pred_best)\n",
    "\n",
    "print(f'Accuracy of Best Model: {accuracy_best}')\n",
    "print(f'Classification Report of Best Model: \\n{report_best}')\n",
    "\n",
    "# Create a DataFrame to display the sentences, their actual labels, and the predicted labels\n",
    "test_data_with_predictions = pd.DataFrame({\n",
    "    'sentence': data.loc[y_test_cleaned.index, 'sentence'],  # Original sentences\n",
    "    'predicted_label': y_pred_best,\n",
    "    'actual_label': y_test_cleaned\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(test_data_with_predictions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a046e858-7084-4ebc-8cc6-a5ae0c319e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18852\\3558919621.py:49: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 1, 'clf__kernel': 'rbf', 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Best Cross-Validation Accuracy: 0.805\n",
      "Accuracy of Best Model: 0.84\n",
      "Classification Report of Best Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85       114\n",
      "           1       0.78      0.88      0.83        86\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.84      0.85      0.84       200\n",
      "weighted avg       0.85      0.84      0.84       200\n",
      "\n",
      "                                                Review  Actual Sentiment  \\\n",
      "204                  Service is friendly and inviting.                 1   \n",
      "71           - Really, really good rice, all the time.                 1   \n",
      "594  It was a pale color instead of nice and char a...                 0   \n",
      "672    As a sushi lover avoid this place by all means.                 0   \n",
      "14   I was disgusted because I was pretty sure that...                 0   \n",
      "64                 We are so glad we found this place.                 1   \n",
      "340       the spaghetti is nothing special whatsoever.                 0   \n",
      "135  Great Subway, in fact it's so good when you co...                 1   \n",
      "350  We sat another ten minutes and finally gave up...                 0   \n",
      "976  too bad cause I know it's family owned, I real...                 0   \n",
      "\n",
      "     Predicted Sentiment  \n",
      "204                    1  \n",
      "71                     1  \n",
      "594                    1  \n",
      "672                    0  \n",
      "14                     0  \n",
      "64                     1  \n",
      "340                    0  \n",
      "135                    1  \n",
      "350                    0  \n",
      "976                    0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Data/Beginner_Reviews_dataset.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Check for missing values and drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add negation words to a separate set\n",
    "negation_words = set([\"not\", \"no\", \"never\", \"none\", \"n't\"])\n",
    "\n",
    "# Function to expand contractions\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "    \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        text = re.sub(contraction, expanded, text)\n",
    "    return text\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Handle negations\n",
    "    processed_words = []\n",
    "    skip_next = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word in negation_words and i + 1 < len(words):\n",
    "            processed_words.append(f\"{word}_{words[i + 1]}\")\n",
    "            skip_next = True\n",
    "        elif skip_next:\n",
    "            skip_next = False\n",
    "        elif word not in stop_words:\n",
    "            processed_words.append(lemmatizer.lemmatize(word))\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "data['cleaned_sentence'] = data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_cleaned = data['cleaned_sentence']\n",
    "y_cleaned = data['label']\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=40)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and SVM classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validation Accuracy: {best_score}')\n",
    "\n",
    "# Predict with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_cleaned)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best = accuracy_score(y_test_cleaned, y_pred_best)\n",
    "report_best = classification_report(y_test_cleaned, y_pred_best)\n",
    "\n",
    "print(f'Accuracy of Best Model: {accuracy_best}')\n",
    "print(f'Classification Report of Best Model: \\n{report_best}')\n",
    "\n",
    "# Create a DataFrame to display the reviews, their actual sentiments, and the predicted sentiments\n",
    "results = pd.DataFrame({\n",
    "    'Review': data.loc[y_test_cleaned.index, 'sentence'],  # Original reviews\n",
    "    'Actual Sentiment': y_test_cleaned,\n",
    "    'Predicted Sentiment': y_pred_best\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36390475-e6cc-44c7-8fdf-b33491b989c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18852\\2554217177.py:49: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 10, 'clf__kernel': 'rbf', 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Best Cross-Validation Accuracy: 0.8125\n",
      "Accuracy of Best Model: 0.815\n",
      "Classification Report of Best Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81        96\n",
      "           1       0.85      0.79      0.82       104\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.82      0.82      0.81       200\n",
      "weighted avg       0.82      0.81      0.82       200\n",
      "\n",
      "                                                Review  Actual Sentiment  \\\n",
      "521                   If you haven't gone here GO NOW!                 1   \n",
      "737  Try them in the airport to experience some tas...                 1   \n",
      "740  The restaurant is very clean and has a family ...                 1   \n",
      "660  I personally love the hummus, pita, baklava, f...                 1   \n",
      "411              Come hungry, leave happy and stuffed!                 1   \n",
      "678      It's a great place and I highly recommend it.                 1   \n",
      "626  Best of luck to the rude and non-customer serv...                 0   \n",
      "513                            Reasonably priced also!                 1   \n",
      "859            Worst food/service I've had in a while.                 0   \n",
      "136            I had a seriously solid breakfast here.                 1   \n",
      "\n",
      "     Predicted Sentiment  \n",
      "521                    1  \n",
      "737                    1  \n",
      "740                    1  \n",
      "660                    1  \n",
      "411                    1  \n",
      "678                    1  \n",
      "626                    0  \n",
      "513                    1  \n",
      "859                    0  \n",
      "136                    1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Data/Beginner_Reviews_dataset.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Check for missing values and drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add negation words to a separate set\n",
    "negation_words = set([\"not\", \"no\", \"never\", \"none\", \"n't\"])\n",
    "\n",
    "# Function to expand contractions\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "    \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        text = re.sub(contraction, expanded, text)\n",
    "    return text\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Handle negations\n",
    "    processed_words = []\n",
    "    skip_next = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word in negation_words and i + 1 < len(words):\n",
    "            processed_words.append(f\"{word}_{words[i + 1]}\")\n",
    "            skip_next = True\n",
    "        elif skip_next:\n",
    "            skip_next = False\n",
    "        elif word not in stop_words:\n",
    "            processed_words.append(lemmatizer.lemmatize(word))\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "data['cleaned_sentence'] = data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_cleaned = data['cleaned_sentence']\n",
    "y_cleaned = data['label']\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and SVM classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validation Accuracy: {best_score}')\n",
    "\n",
    "# Predict with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_cleaned)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best = accuracy_score(y_test_cleaned, y_pred_best)\n",
    "report_best = classification_report(y_test_cleaned, y_pred_best)\n",
    "\n",
    "print(f'Accuracy of Best Model: {accuracy_best}')\n",
    "print(f'Classification Report of Best Model: \\n{report_best}')\n",
    "\n",
    "# Create a DataFrame to display the reviews, their actual sentiments, and the predicted sentiments\n",
    "results = pd.DataFrame({\n",
    "    'Review': data.loc[y_test_cleaned.index, 'sentence'],  # Original reviews\n",
    "    'Actual Sentiment': y_test_cleaned.values,\n",
    "    'Predicted Sentiment': y_pred_best\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38c320ba-6d43-484a-a717-ae74cb4c6af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18852\\3745075862.py:51: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 100, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Best Cross-Validation Accuracy: 0.77875\n",
      "Accuracy of Best Model: 0.72\n",
      "Classification Report of Best Model: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.88      0.75        96\n",
      "           1       0.83      0.58      0.68       104\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.74      0.73      0.72       200\n",
      "weighted avg       0.75      0.72      0.71       200\n",
      "\n",
      "                                                Review  Actual Sentiment  \\\n",
      "521                   If you haven't gone here GO NOW!                 1   \n",
      "737  Try them in the airport to experience some tas...                 1   \n",
      "740  The restaurant is very clean and has a family ...                 1   \n",
      "660  I personally love the hummus, pita, baklava, f...                 1   \n",
      "411              Come hungry, leave happy and stuffed!                 1   \n",
      "678      It's a great place and I highly recommend it.                 1   \n",
      "626  Best of luck to the rude and non-customer serv...                 0   \n",
      "513                            Reasonably priced also!                 1   \n",
      "859            Worst food/service I've had in a while.                 0   \n",
      "136            I had a seriously solid breakfast here.                 1   \n",
      "\n",
      "     Predicted Sentiment  \n",
      "521                    0  \n",
      "737                    1  \n",
      "740                    1  \n",
      "660                    1  \n",
      "411                    1  \n",
      "678                    1  \n",
      "626                    0  \n",
      "513                    0  \n",
      "859                    0  \n",
      "136                    0  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Data/Beginner_Reviews_dataset.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Check for missing values and drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add negation words to a separate set\n",
    "negation_words = set([\"not\", \"no\", \"never\", \"none\", \"n't\"])\n",
    "\n",
    "# Function to expand contractions\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "    \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        text = re.sub(contraction, expanded, text)\n",
    "    return text\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Handle negations\n",
    "    processed_words = []\n",
    "    skip_next = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word in negation_words and i + 1 < len(words):\n",
    "            processed_words.append(f\"{word}_{words[i + 1]}\")\n",
    "            skip_next = True\n",
    "        elif skip_next:\n",
    "            skip_next = False\n",
    "        elif word not in stop_words:\n",
    "            processed_words.append(lemmatizer.lemmatize(word))\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "data['cleaned_sentence'] = data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_cleaned = data['cleaned_sentence']\n",
    "y_cleaned = data['label']\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and Random Forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [None, 10, 20],\n",
    "    'clf__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validation Accuracy: {best_score}')\n",
    "\n",
    "# Predict with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_cleaned)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best = accuracy_score(y_test_cleaned, y_pred_best)\n",
    "report_best = classification_report(y_test_cleaned, y_pred_best)\n",
    "\n",
    "print(f'Accuracy of Best Model: {accuracy_best}')\n",
    "print(f'Classification Report of Best Model: \\n{report_best}')\n",
    "\n",
    "# Create a DataFrame to display the reviews, their actual sentiments, and the predicted sentiments\n",
    "results = pd.DataFrame({\n",
    "    'Review': data.loc[y_test_cleaned.index, 'sentence'],  # Original reviews\n",
    "    'Actual Sentiment': y_test_cleaned.values,\n",
    "    'Predicted Sentiment': y_pred_best\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results.head(10))\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pd.crosstab(results['Actual Sentiment'], results['Predicted Sentiment'], rownames=['Actual'], colnames=['Predicted'], normalize='index'), annot=True, cmap='Blues')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a830f98a-0547-41d5-9495-ee9a90438218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    500\n",
       "0    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
